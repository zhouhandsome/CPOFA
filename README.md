 Layer (type)       Input Shape          Output Shape         Param #
===========================================================================
   Conv2D-1      [[1, 3, 384, 384]]    [1, 768, 24, 24]       590,592
 PatchEmbed-1    [[1, 3, 384, 384]]     [1, 576, 768]            0
   Dropout-1      [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-1     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-1       [[1, 577, 768]]       [1, 577, 2304]       1,771,776
   Dropout-2    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-2       [[1, 577, 768]]       [1, 577, 768]         590,592
   Dropout-3      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-1     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-1      [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-2     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-3       [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-1        [[1, 577, 3072]]      [1, 577, 3072]           0
   Dropout-4      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-4       [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-1        [[1, 577, 768]]       [1, 577, 768]            0
    Block-1       [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-3     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-5       [[1, 577, 768]]       [1, 577, 2304]       1,771,776
   Dropout-5    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-6       [[1, 577, 768]]       [1, 577, 768]         590,592
   Dropout-6      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-2     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-2      [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-4     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-7       [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-2        [[1, 577, 3072]]      [1, 577, 3072]           0
   Dropout-7      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-8       [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-2        [[1, 577, 768]]       [1, 577, 768]            0
    Block-2       [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-5     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-9       [[1, 577, 768]]       [1, 577, 2304]       1,771,776
   Dropout-8    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-10      [[1, 577, 768]]       [1, 577, 768]         590,592
   Dropout-9      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-3     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-3      [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-6     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-11      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-3        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-10      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-12      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-3        [[1, 577, 768]]       [1, 577, 768]            0
    Block-3       [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-7     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-13      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-11    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-14      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-12      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-4     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-4      [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-8     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-15      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-4        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-13      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-16      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-4        [[1, 577, 768]]       [1, 577, 768]            0
    Block-4       [[1, 577, 768]]       [1, 577, 768]            0
  LayerNorm-9     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-17      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-14    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-18      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-15      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-5     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-5      [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-10     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-19      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-5        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-16      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-20      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-5        [[1, 577, 768]]       [1, 577, 768]            0
    Block-5       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-11     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-21      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-17    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-22      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-18      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-6     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-6      [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-12     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-23      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-6        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-19      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-24      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-6        [[1, 577, 768]]       [1, 577, 768]            0
    Block-6       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-13     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-25      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-20    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-26      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-21      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-7     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-7      [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-14     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-27      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-7        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-22      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-28      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-7        [[1, 577, 768]]       [1, 577, 768]            0
    Block-7       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-15     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-29      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-23    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-30      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-24      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-8     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-8      [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-16     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-31      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-8        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-25      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-32      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-8        [[1, 577, 768]]       [1, 577, 768]            0
    Block-8       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-17     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-33      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-26    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-34      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-27      [[1, 577, 768]]       [1, 577, 768]            0
  Attention-9     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-9      [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-18     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-35      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-9        [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-28      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-36      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
     Mlp-9        [[1, 577, 768]]       [1, 577, 768]            0
    Block-9       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-19     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-37      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-29    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-38      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-30      [[1, 577, 768]]       [1, 577, 768]            0
 Attention-10     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-10     [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-20     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-39      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-10       [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-31      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-40      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
    Mlp-10        [[1, 577, 768]]       [1, 577, 768]            0
   Block-10       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-21     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-41      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-32    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-42      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-33      [[1, 577, 768]]       [1, 577, 768]            0
 Attention-11     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-11     [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-22     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-43      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-11       [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-34      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-44      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
    Mlp-11        [[1, 577, 768]]       [1, 577, 768]            0
   Block-11       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-23     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-45      [[1, 577, 768]]       [1, 577, 2304]       1,771,776
  Dropout-35    [[1, 12, 577, 577]]   [1, 12, 577, 577]          0
   Linear-46      [[1, 577, 768]]       [1, 577, 768]         590,592
  Dropout-36      [[1, 577, 768]]       [1, 577, 768]            0
 Attention-12     [[1, 577, 768]]       [1, 577, 768]            0
  Identity-12     [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-24     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-47      [[1, 577, 768]]       [1, 577, 3072]       2,362,368
    GELU-12       [[1, 577, 3072]]      [1, 577, 3072]           0
  Dropout-37      [[1, 577, 768]]       [1, 577, 768]            0
   Linear-48      [[1, 577, 3072]]      [1, 577, 768]        2,360,064
    Mlp-12        [[1, 577, 768]]       [1, 577, 768]            0
   Block-12       [[1, 577, 768]]       [1, 577, 768]            0
 LayerNorm-25     [[1, 577, 768]]       [1, 577, 768]          1,536
   Linear-49         [[1, 768]]           [1, 1000]           769,000
===========================================================================
Total params: 86,415,592
Trainable params: 86,415,592
Non-trainable params: 0
---------------------------------------------------------------------------
Input size (MB): 1.69
Forward/backward pass size (MB): 1231.26
Params size (MB): 329.65
Estimated Total Size (MB): 1562.60
---------------------------------------------------------------------------
## AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

当图片数据集大的时候，transfromers会变得很强

### VIT(Visual Transfromer)


#### 图像卷积的计算，手推

#### 自注意力模型


界面设置
# 数据获取，
# 数据预处理 ===》数据集的一些信息
# 模型训练  ===》 训练损失图，对比图
# 模型推理  ===》 结果，准确率，混淆矩阵